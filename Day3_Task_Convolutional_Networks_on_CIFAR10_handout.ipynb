{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Day3_Task_Convolutional_Networks_on_CIFAR10_handout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSHhdguTHm0N"
      },
      "source": [
        "# Task: CIFAR-10 classification\n",
        "\n",
        "The [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "\n",
        "> \"consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        ">The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\"\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1wlfkvZgS0oBDwxKicWmYgtsJmP3IcMdj\">\n",
        "\n",
        "### Categories:\n",
        "\n",
        "- airplane \t\t\t\t\t\t\t\t\t\t\n",
        "- automobile \t\t\t\t\t\t\t\t\t\t\n",
        "- bird \t\t\t\t\t\t\t\t\t\t\n",
        "- cat \t\t\t\t\t\t\t\t\t\t\n",
        "- deer \t\t\t\t\t\t\t\t\t\t\n",
        "- dog \t\t\t\t\t\t\t\t\t\t\n",
        "- frog \t\t\t\t\t\t\t\t\t\t\n",
        "- horse \t\t\t\t\t\t\t\t\t\t\n",
        "- ship \t\t\t\t\t\t\t\t\t\t\n",
        "- truck"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIX2ehpiHm0S"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:01.205778Z",
          "start_time": "2019-05-01T08:39:00.058811Z"
        },
        "id": "sTT_MyHltNm4"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adadelta, Adam, SGD\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.initializers import glorot_normal\n",
        "# from tensorboardcolab import TensorBoardColab \n",
        "\n",
        "# Fix seeds for (hopefully) reproducible results\n",
        "from numpy.random import seed\n",
        "seed(14)\n",
        "tf.random.set_seed(19)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d9_yHMEtNnG"
      },
      "source": [
        "Download the data if necessary and load it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:36.264800Z",
          "start_time": "2019-05-01T08:39:01.207234Z"
        },
        "id": "jFmPQL0xM_4p",
        "outputId": "ce6d1005-df2e-4a02-f989-2eb5431ff34e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "train, test = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "train_images, train_labels = train\n",
        "\n",
        "valid_test_images, valid_test_labels = test\n",
        "\n",
        "train_images = train_images / 255.\n",
        "\n",
        "valid_test_images = valid_test_images / 255.\n",
        "\n",
        "valid_images = valid_test_images[:5000]\n",
        "valid_labels = valid_test_labels[:5000]\n",
        "test_images = valid_test_images[5000:]\n",
        "test_labels = valid_test_labels[5000:]\n",
        "\n",
        "print(train_images.shape, valid_images.shape, test_images.shape)\n",
        "print(train_labels.shape, valid_labels.shape, test_labels.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "(50000, 32, 32, 3) (5000, 32, 32, 3) (5000, 32, 32, 3)\n",
            "(50000, 1) (5000, 1) (5000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxyeFqibHm05"
      },
      "source": [
        "# Model\n",
        "\n",
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:36.268218Z",
          "start_time": "2019-05-01T08:39:36.266265Z"
        },
        "id": "S075_4a6tNna"
      },
      "source": [
        "n_classes = 10"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPI8Q5W8E95m"
      },
      "source": [
        "# TASK - Hyperparameters\n",
        "# Fill in the initial values!\n",
        "# Later, experiment!\n",
        "#############################\n",
        "\n",
        "\n",
        "# dropout - Something between 0.0 < dropout_rate < 1.0, think in \"tens of percentages\" as default\n",
        "# dropout rate for conv layers\n",
        "dropout_rate_1 = 0.63\n",
        "# dropout rate for fully connected layers\n",
        "dropout_rate_2 = 0.67\n",
        "\n",
        "# Choose an appropriate batch size for the training!\n",
        "batch_size = 512\n",
        "\n",
        "# Choose an appropriate number of epochs\n",
        "epoch_count = 500\n",
        "\n",
        "# These are the default parameters, you can experiment with learning rates, schedules, ..."
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACdo3SEitNnl"
      },
      "source": [
        "## Network\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:36.404889Z",
          "start_time": "2019-05-01T08:39:36.282271Z"
        },
        "id": "o8OgyCFaHm1H",
        "outputId": "6a288e8e-67fd-42e3-ae15-5d4d86f922f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        }
      },
      "source": [
        "# adapted from https://github.com/jtopor/CUNY-MSDA-661/blob/master/CIFAR-CNN/TF-Layers-CIFAR-GITHUB-v3.py\n",
        "\n",
        "\n",
        "tf.compat.v1.reset_default_graph() # It's good practice to clean and reset everything\n",
        "clear_session()          # even using Keras\n",
        "\n",
        "\n",
        "# WE USE FUNCTIONAL API!\n",
        "# (Could be different, but not now...)\n",
        "\n",
        "\n",
        "\n",
        "# Model\n",
        "#######\n",
        "\n",
        "# Define the input!\n",
        "# Remember, we have pictures with 32x32 pixels and 3 color channels\n",
        "# Disregard batch size, Keras will do that for us.\n",
        "x = Input(shape=(32, 32, 3))\n",
        "\n",
        "# Convolutional Layer #1: (batch_size, 32, 32, 3) -> (batch_size, 32, 32, 64)\n",
        "# Define a \"normal\" convolutional layer for images (not a single sequence, so ?D)\n",
        "# There should be 64 convolutional units\n",
        "# The kernel should be 5 in width and heigth\n",
        "# There should be padding so that the input and output dimensions would be equivalent\n",
        "# The non-linearity should be ReLU\n",
        "conv1 = Conv2D(64, 5, strides=(1, 1), padding=\"same\", activation='relu', use_bias=True, kernel_initializer=glorot_normal(seed=19), bias_initializer=\"zeros\")(x)\n",
        " \n",
        "# Pooling Layer #1: (batch_size, 32, 32, 64) -> (batch_size, 16, 16, 64)\n",
        "# Define a maximum based pooling layer with appropriate dimensions\n",
        "# The pooling size should be 2,2 and stride 2\n",
        "pool1 = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"valid\")(conv1)\n",
        "\n",
        "# Define a dropout layer with using the first dropout rate parameter\n",
        "dropout1 = Dropout(dropout_rate_1)(pool1)\n",
        "\n",
        "# Convolutional Layer #2: (batch_size, 16, 16, 64) -> (batch_size, 16, 16, 64)\n",
        "# Repeat the prior conv layer\n",
        "# Watch for the right input\n",
        "conv2 = Conv2D(64, 5, strides=(1, 1), padding=\"same\", activation='relu', use_bias=True, kernel_initializer=glorot_normal(seed=19), bias_initializer=\"zeros\")(dropout1)\n",
        "  \n",
        "# Pooling Layer #2: (batch_size, 16, 16, 64) -> (batch_size, 8, 8, 64)\n",
        "# Repeat the prior pooling layer\n",
        "# Watch for the right input\n",
        "pool2 = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"valid\")(conv2)\n",
        "\n",
        "# Define a dropout layer with using the FIRST dropout rate parameter\n",
        "dropout2 = Dropout(dropout_rate_1)(pool2)\n",
        "\n",
        "# Convert tensors into vectors: (batch_size, 8, 8, 64) -> (batch_size, 4096)\n",
        "# Use a single KERAS function, NO numpy or reshape magic!\n",
        "# Hint: the result is not 2D but \"flat\"\n",
        "pool2_flat = Flatten()(dropout2)\n",
        "\n",
        "# Fully connected Layer #1: (batch_size, 4096)-> (batch_size, 512)\n",
        "# Define a fully connected layer with 512 nodes and ReLU\n",
        "dense1 = Dense(512,activation='relu', use_bias=True,kernel_initializer=glorot_normal(seed=19),bias_initializer=\"zeros\")(pool2_flat)\n",
        "\n",
        "# Define a dropout layer with using the SECOND dropout rate parameter\n",
        "dropout3 = Dropout(dropout_rate_2)(dense1)\n",
        "\n",
        "# Dense Layer #1: (batch_size, 512)-> (batch_size, 256)\n",
        "# Define a fully connected layer with 256 nodes and ReLU\n",
        "dense2 = Dense(256,activation='relu', use_bias=True,kernel_initializer=glorot_normal(seed=19),bias_initializer=\"zeros\")(dropout3)\n",
        "\n",
        "\n",
        "# Define a dropout layer with using the SECOND dropout rate parameter\n",
        "dropout4 = Dropout(dropout_rate_2)(dense2)\n",
        "\n",
        "# Logits layer: (batch_size, 256) -> (batch_size, 10)\n",
        "# Define a fully connected layer with ??? nodes\n",
        "# Think about it, what shape should the output be?\n",
        "# What activation?\n",
        "# Think about it: we are in a classification problem!\n",
        "predictions = Dense(n_classes, activation='softmax')(dropout4)\n",
        "\n",
        "# Full model\n",
        "# Instantiate (initialize) the model with inputs and outputs\n",
        "model = Model(x, predictions)\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 32, 32, 64)        4864      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 64)        102464    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               2097664   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 2,338,890\n",
            "Trainable params: 2,338,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGKQ-8bmtNn1"
      },
      "source": [
        "## Loss, optimization and compilation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:39:36.473837Z",
          "start_time": "2019-05-01T08:39:36.407595Z"
        },
        "id": "j1UmvSzLHm1R"
      },
      "source": [
        "# Loss \n",
        "\n",
        "loss = sparse_categorical_crossentropy # we use this cross entropy variant as the input is not \n",
        "                                       # one-hot encoded\n",
        "\n",
        "# Optimizer\n",
        "# Choose an optimizer - adaptive ones work well here\n",
        "optimizer = Adam(learning_rate=0.00075) #0.00075\n",
        " \n",
        "# Compilation\n",
        "#############\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkPe16fDtNoJ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:51:59.029061Z",
          "start_time": "2019-05-01T08:39:36.475186Z"
        },
        "id": "zZ1d14lFtNoM",
        "outputId": "f61d48bf-3623-4ef4-b136-90634489f997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "history = model.fit(x=train_images, y=train_labels,\n",
        "                    validation_data=(valid_images, valid_labels),\n",
        "                    epochs=epoch_count,\n",
        "                    batch_size=batch_size)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "98/98 [==============================] - 3s 33ms/step - loss: 2.2141 - accuracy: 0.1568 - val_loss: 2.0262 - val_accuracy: 0.2522\n",
            "Epoch 2/500\n",
            "98/98 [==============================] - 3s 30ms/step - loss: 1.8618 - accuracy: 0.2968 - val_loss: 1.6554 - val_accuracy: 0.4196\n",
            "Epoch 3/500\n",
            "98/98 [==============================] - 3s 30ms/step - loss: 1.6706 - accuracy: 0.3775 - val_loss: 1.5094 - val_accuracy: 0.4698\n",
            "Epoch 4/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.5637 - accuracy: 0.4278 - val_loss: 1.4008 - val_accuracy: 0.5068\n",
            "Epoch 5/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.4932 - accuracy: 0.4567 - val_loss: 1.3678 - val_accuracy: 0.5078\n",
            "Epoch 6/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.4369 - accuracy: 0.4819 - val_loss: 1.3071 - val_accuracy: 0.5466\n",
            "Epoch 7/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.3856 - accuracy: 0.4982 - val_loss: 1.2425 - val_accuracy: 0.5576\n",
            "Epoch 8/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.3346 - accuracy: 0.5205 - val_loss: 1.1890 - val_accuracy: 0.5868\n",
            "Epoch 9/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.3083 - accuracy: 0.5303 - val_loss: 1.1629 - val_accuracy: 0.6010\n",
            "Epoch 10/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.2788 - accuracy: 0.5464 - val_loss: 1.1288 - val_accuracy: 0.6090\n",
            "Epoch 11/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.2506 - accuracy: 0.5556 - val_loss: 1.0732 - val_accuracy: 0.6332\n",
            "Epoch 12/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.2228 - accuracy: 0.5668 - val_loss: 1.1153 - val_accuracy: 0.6152\n",
            "Epoch 13/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.1998 - accuracy: 0.5764 - val_loss: 1.0384 - val_accuracy: 0.6422\n",
            "Epoch 14/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.1807 - accuracy: 0.5861 - val_loss: 1.0317 - val_accuracy: 0.6408\n",
            "Epoch 15/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.1548 - accuracy: 0.5946 - val_loss: 1.0072 - val_accuracy: 0.6558\n",
            "Epoch 16/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.1442 - accuracy: 0.6004 - val_loss: 1.0114 - val_accuracy: 0.6514\n",
            "Epoch 17/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.1250 - accuracy: 0.6059 - val_loss: 1.0029 - val_accuracy: 0.6578\n",
            "Epoch 18/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.1144 - accuracy: 0.6105 - val_loss: 0.9636 - val_accuracy: 0.6632\n",
            "Epoch 19/500\n",
            "98/98 [==============================] - 3s 30ms/step - loss: 1.1009 - accuracy: 0.6143 - val_loss: 0.9346 - val_accuracy: 0.6818\n",
            "Epoch 20/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.0846 - accuracy: 0.6226 - val_loss: 0.9401 - val_accuracy: 0.6728\n",
            "Epoch 21/500\n",
            "98/98 [==============================] - 3s 30ms/step - loss: 1.0734 - accuracy: 0.6263 - val_loss: 0.9023 - val_accuracy: 0.6862\n",
            "Epoch 22/500\n",
            "98/98 [==============================] - 3s 30ms/step - loss: 1.0593 - accuracy: 0.6305 - val_loss: 0.9150 - val_accuracy: 0.6796\n",
            "Epoch 23/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.0476 - accuracy: 0.6385 - val_loss: 0.8706 - val_accuracy: 0.6982\n",
            "Epoch 24/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.0325 - accuracy: 0.6404 - val_loss: 0.8638 - val_accuracy: 0.7102\n",
            "Epoch 25/500\n",
            "98/98 [==============================] - 3s 30ms/step - loss: 1.0332 - accuracy: 0.6413 - val_loss: 0.8563 - val_accuracy: 0.7074\n",
            "Epoch 26/500\n",
            "98/98 [==============================] - 3s 30ms/step - loss: 1.0159 - accuracy: 0.6498 - val_loss: 0.8620 - val_accuracy: 0.7070\n",
            "Epoch 27/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 1.0157 - accuracy: 0.6497 - val_loss: 0.8681 - val_accuracy: 0.6974\n",
            "Epoch 28/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9972 - accuracy: 0.6533 - val_loss: 0.8650 - val_accuracy: 0.6980\n",
            "Epoch 29/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9876 - accuracy: 0.6580 - val_loss: 0.8270 - val_accuracy: 0.7218\n",
            "Epoch 30/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9914 - accuracy: 0.6557 - val_loss: 0.8708 - val_accuracy: 0.6984\n",
            "Epoch 31/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9760 - accuracy: 0.6620 - val_loss: 0.8270 - val_accuracy: 0.7114\n",
            "Epoch 32/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9715 - accuracy: 0.6647 - val_loss: 0.8051 - val_accuracy: 0.7220\n",
            "Epoch 33/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9691 - accuracy: 0.6637 - val_loss: 0.8036 - val_accuracy: 0.7234\n",
            "Epoch 34/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9592 - accuracy: 0.6698 - val_loss: 0.7892 - val_accuracy: 0.7312\n",
            "Epoch 35/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9490 - accuracy: 0.6713 - val_loss: 0.7902 - val_accuracy: 0.7320\n",
            "Epoch 36/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9370 - accuracy: 0.6752 - val_loss: 0.7971 - val_accuracy: 0.7242\n",
            "Epoch 37/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9375 - accuracy: 0.6757 - val_loss: 0.7824 - val_accuracy: 0.7306\n",
            "Epoch 38/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9290 - accuracy: 0.6812 - val_loss: 0.7902 - val_accuracy: 0.7268\n",
            "Epoch 39/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9271 - accuracy: 0.6815 - val_loss: 0.8156 - val_accuracy: 0.7150\n",
            "Epoch 40/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9334 - accuracy: 0.6771 - val_loss: 0.7684 - val_accuracy: 0.7390\n",
            "Epoch 41/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9256 - accuracy: 0.6812 - val_loss: 0.8144 - val_accuracy: 0.7142\n",
            "Epoch 42/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9106 - accuracy: 0.6839 - val_loss: 0.7746 - val_accuracy: 0.7356\n",
            "Epoch 43/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9053 - accuracy: 0.6898 - val_loss: 0.7725 - val_accuracy: 0.7288\n",
            "Epoch 44/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.9045 - accuracy: 0.6897 - val_loss: 0.7608 - val_accuracy: 0.7368\n",
            "Epoch 45/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8968 - accuracy: 0.6916 - val_loss: 0.8203 - val_accuracy: 0.7192\n",
            "Epoch 46/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8986 - accuracy: 0.6902 - val_loss: 0.7399 - val_accuracy: 0.7468\n",
            "Epoch 47/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8872 - accuracy: 0.6949 - val_loss: 0.7329 - val_accuracy: 0.7506\n",
            "Epoch 48/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8838 - accuracy: 0.6940 - val_loss: 0.7432 - val_accuracy: 0.7420\n",
            "Epoch 49/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8844 - accuracy: 0.6977 - val_loss: 0.7424 - val_accuracy: 0.7422\n",
            "Epoch 50/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8845 - accuracy: 0.6949 - val_loss: 0.7554 - val_accuracy: 0.7424\n",
            "Epoch 51/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8853 - accuracy: 0.6943 - val_loss: 0.7765 - val_accuracy: 0.7300\n",
            "Epoch 52/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8704 - accuracy: 0.7012 - val_loss: 0.7421 - val_accuracy: 0.7422\n",
            "Epoch 53/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8704 - accuracy: 0.6990 - val_loss: 0.7312 - val_accuracy: 0.7494\n",
            "Epoch 54/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8619 - accuracy: 0.7024 - val_loss: 0.7028 - val_accuracy: 0.7632\n",
            "Epoch 55/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8751 - accuracy: 0.7025 - val_loss: 0.7375 - val_accuracy: 0.7432\n",
            "Epoch 56/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8653 - accuracy: 0.7016 - val_loss: 0.7104 - val_accuracy: 0.7556\n",
            "Epoch 57/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8577 - accuracy: 0.7040 - val_loss: 0.7296 - val_accuracy: 0.7502\n",
            "Epoch 58/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8554 - accuracy: 0.7047 - val_loss: 0.7311 - val_accuracy: 0.7504\n",
            "Epoch 59/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8555 - accuracy: 0.7045 - val_loss: 0.7343 - val_accuracy: 0.7568\n",
            "Epoch 60/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8459 - accuracy: 0.7095 - val_loss: 0.7051 - val_accuracy: 0.7590\n",
            "Epoch 61/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8503 - accuracy: 0.7089 - val_loss: 0.7047 - val_accuracy: 0.7600\n",
            "Epoch 62/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8399 - accuracy: 0.7111 - val_loss: 0.7009 - val_accuracy: 0.7610\n",
            "Epoch 63/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8334 - accuracy: 0.7151 - val_loss: 0.6960 - val_accuracy: 0.7560\n",
            "Epoch 64/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8376 - accuracy: 0.7122 - val_loss: 0.7065 - val_accuracy: 0.7608\n",
            "Epoch 65/500\n",
            "98/98 [==============================] - 3s 30ms/step - loss: 0.8318 - accuracy: 0.7149 - val_loss: 0.6897 - val_accuracy: 0.7602\n",
            "Epoch 66/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8283 - accuracy: 0.7147 - val_loss: 0.7046 - val_accuracy: 0.7580\n",
            "Epoch 67/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8246 - accuracy: 0.7170 - val_loss: 0.6985 - val_accuracy: 0.7610\n",
            "Epoch 68/500\n",
            "98/98 [==============================] - 3s 30ms/step - loss: 0.8213 - accuracy: 0.7174 - val_loss: 0.6830 - val_accuracy: 0.7666\n",
            "Epoch 69/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8241 - accuracy: 0.7189 - val_loss: 0.6885 - val_accuracy: 0.7664\n",
            "Epoch 70/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8170 - accuracy: 0.7194 - val_loss: 0.6926 - val_accuracy: 0.7668\n",
            "Epoch 71/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8195 - accuracy: 0.7180 - val_loss: 0.7101 - val_accuracy: 0.7568\n",
            "Epoch 72/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8178 - accuracy: 0.7191 - val_loss: 0.7208 - val_accuracy: 0.7522\n",
            "Epoch 73/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8118 - accuracy: 0.7233 - val_loss: 0.7165 - val_accuracy: 0.7536\n",
            "Epoch 74/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8111 - accuracy: 0.7207 - val_loss: 0.6933 - val_accuracy: 0.7624\n",
            "Epoch 75/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8033 - accuracy: 0.7232 - val_loss: 0.6843 - val_accuracy: 0.7698\n",
            "Epoch 76/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8176 - accuracy: 0.7195 - val_loss: 0.7077 - val_accuracy: 0.7594\n",
            "Epoch 77/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8048 - accuracy: 0.7206 - val_loss: 0.6841 - val_accuracy: 0.7724\n",
            "Epoch 78/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7996 - accuracy: 0.7269 - val_loss: 0.6960 - val_accuracy: 0.7616\n",
            "Epoch 79/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8077 - accuracy: 0.7232 - val_loss: 0.6800 - val_accuracy: 0.7740\n",
            "Epoch 80/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8005 - accuracy: 0.7256 - val_loss: 0.6755 - val_accuracy: 0.7678\n",
            "Epoch 81/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7983 - accuracy: 0.7240 - val_loss: 0.6848 - val_accuracy: 0.7668\n",
            "Epoch 82/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.8030 - accuracy: 0.7254 - val_loss: 0.6870 - val_accuracy: 0.7610\n",
            "Epoch 83/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7904 - accuracy: 0.7296 - val_loss: 0.7131 - val_accuracy: 0.7574\n",
            "Epoch 84/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7969 - accuracy: 0.7274 - val_loss: 0.6877 - val_accuracy: 0.7684\n",
            "Epoch 85/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7889 - accuracy: 0.7280 - val_loss: 0.6821 - val_accuracy: 0.7652\n",
            "Epoch 86/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7810 - accuracy: 0.7346 - val_loss: 0.7085 - val_accuracy: 0.7582\n",
            "Epoch 87/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7851 - accuracy: 0.7330 - val_loss: 0.6737 - val_accuracy: 0.7722\n",
            "Epoch 88/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7775 - accuracy: 0.7327 - val_loss: 0.6651 - val_accuracy: 0.7742\n",
            "Epoch 89/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7844 - accuracy: 0.7315 - val_loss: 0.6940 - val_accuracy: 0.7620\n",
            "Epoch 90/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7843 - accuracy: 0.7319 - val_loss: 0.6865 - val_accuracy: 0.7710\n",
            "Epoch 91/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7772 - accuracy: 0.7346 - val_loss: 0.7033 - val_accuracy: 0.7590\n",
            "Epoch 92/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7785 - accuracy: 0.7311 - val_loss: 0.6764 - val_accuracy: 0.7694\n",
            "Epoch 93/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7730 - accuracy: 0.7347 - val_loss: 0.6817 - val_accuracy: 0.7620\n",
            "Epoch 94/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7764 - accuracy: 0.7315 - val_loss: 0.6650 - val_accuracy: 0.7710\n",
            "Epoch 95/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7648 - accuracy: 0.7363 - val_loss: 0.6722 - val_accuracy: 0.7706\n",
            "Epoch 96/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7646 - accuracy: 0.7375 - val_loss: 0.6737 - val_accuracy: 0.7712\n",
            "Epoch 97/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7702 - accuracy: 0.7346 - val_loss: 0.6679 - val_accuracy: 0.7722\n",
            "Epoch 98/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7600 - accuracy: 0.7373 - val_loss: 0.6682 - val_accuracy: 0.7742\n",
            "Epoch 99/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7677 - accuracy: 0.7385 - val_loss: 0.6648 - val_accuracy: 0.7700\n",
            "Epoch 100/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7676 - accuracy: 0.7357 - val_loss: 0.6532 - val_accuracy: 0.7786\n",
            "Epoch 101/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7552 - accuracy: 0.7423 - val_loss: 0.6721 - val_accuracy: 0.7676\n",
            "Epoch 102/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7713 - accuracy: 0.7357 - val_loss: 0.6664 - val_accuracy: 0.7748\n",
            "Epoch 103/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7585 - accuracy: 0.7397 - val_loss: 0.6594 - val_accuracy: 0.7760\n",
            "Epoch 104/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7552 - accuracy: 0.7411 - val_loss: 0.6454 - val_accuracy: 0.7832\n",
            "Epoch 105/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7502 - accuracy: 0.7432 - val_loss: 0.6545 - val_accuracy: 0.7788\n",
            "Epoch 106/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7549 - accuracy: 0.7436 - val_loss: 0.6525 - val_accuracy: 0.7786\n",
            "Epoch 107/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7550 - accuracy: 0.7400 - val_loss: 0.6625 - val_accuracy: 0.7748\n",
            "Epoch 108/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7500 - accuracy: 0.7432 - val_loss: 0.6521 - val_accuracy: 0.7792\n",
            "Epoch 109/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7605 - accuracy: 0.7387 - val_loss: 0.6536 - val_accuracy: 0.7822\n",
            "Epoch 110/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7532 - accuracy: 0.7454 - val_loss: 0.6532 - val_accuracy: 0.7832\n",
            "Epoch 111/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7387 - accuracy: 0.7458 - val_loss: 0.6492 - val_accuracy: 0.7820\n",
            "Epoch 112/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7449 - accuracy: 0.7462 - val_loss: 0.6505 - val_accuracy: 0.7788\n",
            "Epoch 113/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7475 - accuracy: 0.7442 - val_loss: 0.6611 - val_accuracy: 0.7790\n",
            "Epoch 114/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7317 - accuracy: 0.7471 - val_loss: 0.6645 - val_accuracy: 0.7782\n",
            "Epoch 115/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7309 - accuracy: 0.7489 - val_loss: 0.6850 - val_accuracy: 0.7650\n",
            "Epoch 116/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7377 - accuracy: 0.7486 - val_loss: 0.6465 - val_accuracy: 0.7798\n",
            "Epoch 117/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7405 - accuracy: 0.7460 - val_loss: 0.6554 - val_accuracy: 0.7744\n",
            "Epoch 118/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7372 - accuracy: 0.7449 - val_loss: 0.6483 - val_accuracy: 0.7820\n",
            "Epoch 119/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7362 - accuracy: 0.7466 - val_loss: 0.6499 - val_accuracy: 0.7818\n",
            "Epoch 120/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7342 - accuracy: 0.7487 - val_loss: 0.6439 - val_accuracy: 0.7784\n",
            "Epoch 121/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7245 - accuracy: 0.7515 - val_loss: 0.6505 - val_accuracy: 0.7816\n",
            "Epoch 122/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7442 - accuracy: 0.7463 - val_loss: 0.6433 - val_accuracy: 0.7814\n",
            "Epoch 123/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7301 - accuracy: 0.7508 - val_loss: 0.6431 - val_accuracy: 0.7822\n",
            "Epoch 124/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7378 - accuracy: 0.7486 - val_loss: 0.6397 - val_accuracy: 0.7860\n",
            "Epoch 125/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7262 - accuracy: 0.7524 - val_loss: 0.6434 - val_accuracy: 0.7844\n",
            "Epoch 126/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7229 - accuracy: 0.7504 - val_loss: 0.6475 - val_accuracy: 0.7822\n",
            "Epoch 127/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7290 - accuracy: 0.7507 - val_loss: 0.6412 - val_accuracy: 0.7808\n",
            "Epoch 128/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7272 - accuracy: 0.7504 - val_loss: 0.6393 - val_accuracy: 0.7892\n",
            "Epoch 129/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7240 - accuracy: 0.7529 - val_loss: 0.6500 - val_accuracy: 0.7780\n",
            "Epoch 130/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7145 - accuracy: 0.7531 - val_loss: 0.6444 - val_accuracy: 0.7796\n",
            "Epoch 131/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7315 - accuracy: 0.7501 - val_loss: 0.6842 - val_accuracy: 0.7684\n",
            "Epoch 132/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7185 - accuracy: 0.7517 - val_loss: 0.6540 - val_accuracy: 0.7758\n",
            "Epoch 133/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7271 - accuracy: 0.7516 - val_loss: 0.6354 - val_accuracy: 0.7844\n",
            "Epoch 134/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7210 - accuracy: 0.7512 - val_loss: 0.6551 - val_accuracy: 0.7810\n",
            "Epoch 135/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7175 - accuracy: 0.7548 - val_loss: 0.6650 - val_accuracy: 0.7740\n",
            "Epoch 136/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7119 - accuracy: 0.7581 - val_loss: 0.6422 - val_accuracy: 0.7868\n",
            "Epoch 137/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7098 - accuracy: 0.7576 - val_loss: 0.6335 - val_accuracy: 0.7856\n",
            "Epoch 138/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7165 - accuracy: 0.7569 - val_loss: 0.6605 - val_accuracy: 0.7710\n",
            "Epoch 139/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7023 - accuracy: 0.7572 - val_loss: 0.6227 - val_accuracy: 0.7928\n",
            "Epoch 140/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.7024 - accuracy: 0.7621 - val_loss: 0.6306 - val_accuracy: 0.7894\n",
            "Epoch 141/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7060 - accuracy: 0.7579 - val_loss: 0.6410 - val_accuracy: 0.7860\n",
            "Epoch 142/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7021 - accuracy: 0.7582 - val_loss: 0.6155 - val_accuracy: 0.7928\n",
            "Epoch 143/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7153 - accuracy: 0.7570 - val_loss: 0.6414 - val_accuracy: 0.7820\n",
            "Epoch 144/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7120 - accuracy: 0.7582 - val_loss: 0.6317 - val_accuracy: 0.7852\n",
            "Epoch 145/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7039 - accuracy: 0.7570 - val_loss: 0.6349 - val_accuracy: 0.7818\n",
            "Epoch 146/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6955 - accuracy: 0.7610 - val_loss: 0.6306 - val_accuracy: 0.7886\n",
            "Epoch 147/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7105 - accuracy: 0.7554 - val_loss: 0.6299 - val_accuracy: 0.7872\n",
            "Epoch 148/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6991 - accuracy: 0.7612 - val_loss: 0.6342 - val_accuracy: 0.7810\n",
            "Epoch 149/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6969 - accuracy: 0.7602 - val_loss: 0.6323 - val_accuracy: 0.7908\n",
            "Epoch 150/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7025 - accuracy: 0.7597 - val_loss: 0.6299 - val_accuracy: 0.7928\n",
            "Epoch 151/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7004 - accuracy: 0.7593 - val_loss: 0.6261 - val_accuracy: 0.7862\n",
            "Epoch 152/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6955 - accuracy: 0.7621 - val_loss: 0.6213 - val_accuracy: 0.7882\n",
            "Epoch 153/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.7059 - accuracy: 0.7599 - val_loss: 0.6269 - val_accuracy: 0.7894\n",
            "Epoch 154/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6959 - accuracy: 0.7621 - val_loss: 0.6352 - val_accuracy: 0.7858\n",
            "Epoch 155/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6931 - accuracy: 0.7624 - val_loss: 0.6187 - val_accuracy: 0.7896\n",
            "Epoch 156/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6888 - accuracy: 0.7649 - val_loss: 0.6234 - val_accuracy: 0.7890\n",
            "Epoch 157/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6853 - accuracy: 0.7641 - val_loss: 0.6239 - val_accuracy: 0.7870\n",
            "Epoch 158/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6994 - accuracy: 0.7608 - val_loss: 0.6221 - val_accuracy: 0.7888\n",
            "Epoch 159/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6886 - accuracy: 0.7633 - val_loss: 0.6259 - val_accuracy: 0.7866\n",
            "Epoch 160/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6934 - accuracy: 0.7632 - val_loss: 0.6258 - val_accuracy: 0.7860\n",
            "Epoch 161/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6938 - accuracy: 0.7650 - val_loss: 0.6325 - val_accuracy: 0.7902\n",
            "Epoch 162/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6884 - accuracy: 0.7650 - val_loss: 0.6083 - val_accuracy: 0.7934\n",
            "Epoch 163/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6904 - accuracy: 0.7664 - val_loss: 0.6438 - val_accuracy: 0.7786\n",
            "Epoch 164/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6868 - accuracy: 0.7633 - val_loss: 0.6141 - val_accuracy: 0.7924\n",
            "Epoch 165/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6916 - accuracy: 0.7632 - val_loss: 0.6266 - val_accuracy: 0.7886\n",
            "Epoch 166/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6781 - accuracy: 0.7678 - val_loss: 0.6159 - val_accuracy: 0.7888\n",
            "Epoch 167/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6837 - accuracy: 0.7671 - val_loss: 0.6201 - val_accuracy: 0.7912\n",
            "Epoch 168/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6843 - accuracy: 0.7656 - val_loss: 0.6372 - val_accuracy: 0.7866\n",
            "Epoch 169/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6907 - accuracy: 0.7652 - val_loss: 0.6128 - val_accuracy: 0.7934\n",
            "Epoch 170/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6874 - accuracy: 0.7659 - val_loss: 0.6084 - val_accuracy: 0.7920\n",
            "Epoch 171/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6789 - accuracy: 0.7680 - val_loss: 0.6108 - val_accuracy: 0.7908\n",
            "Epoch 172/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6752 - accuracy: 0.7672 - val_loss: 0.6102 - val_accuracy: 0.7950\n",
            "Epoch 173/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6770 - accuracy: 0.7674 - val_loss: 0.6095 - val_accuracy: 0.7882\n",
            "Epoch 174/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6806 - accuracy: 0.7680 - val_loss: 0.6066 - val_accuracy: 0.7986\n",
            "Epoch 175/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6772 - accuracy: 0.7688 - val_loss: 0.6042 - val_accuracy: 0.7988\n",
            "Epoch 176/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6758 - accuracy: 0.7687 - val_loss: 0.6433 - val_accuracy: 0.7864\n",
            "Epoch 177/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6805 - accuracy: 0.7685 - val_loss: 0.6295 - val_accuracy: 0.7868\n",
            "Epoch 178/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6827 - accuracy: 0.7662 - val_loss: 0.6085 - val_accuracy: 0.7988\n",
            "Epoch 179/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6787 - accuracy: 0.7664 - val_loss: 0.6337 - val_accuracy: 0.7882\n",
            "Epoch 180/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6716 - accuracy: 0.7700 - val_loss: 0.6145 - val_accuracy: 0.7954\n",
            "Epoch 181/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6732 - accuracy: 0.7703 - val_loss: 0.6065 - val_accuracy: 0.7964\n",
            "Epoch 182/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6715 - accuracy: 0.7714 - val_loss: 0.6175 - val_accuracy: 0.7908\n",
            "Epoch 183/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6757 - accuracy: 0.7696 - val_loss: 0.6116 - val_accuracy: 0.7948\n",
            "Epoch 184/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6708 - accuracy: 0.7712 - val_loss: 0.6003 - val_accuracy: 0.7994\n",
            "Epoch 185/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6758 - accuracy: 0.7685 - val_loss: 0.6049 - val_accuracy: 0.7952\n",
            "Epoch 186/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6730 - accuracy: 0.7682 - val_loss: 0.6108 - val_accuracy: 0.7946\n",
            "Epoch 187/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6709 - accuracy: 0.7706 - val_loss: 0.6302 - val_accuracy: 0.7900\n",
            "Epoch 188/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6696 - accuracy: 0.7717 - val_loss: 0.6186 - val_accuracy: 0.7874\n",
            "Epoch 189/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6687 - accuracy: 0.7709 - val_loss: 0.6033 - val_accuracy: 0.7954\n",
            "Epoch 190/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6691 - accuracy: 0.7699 - val_loss: 0.6215 - val_accuracy: 0.7922\n",
            "Epoch 191/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6732 - accuracy: 0.7689 - val_loss: 0.6161 - val_accuracy: 0.7920\n",
            "Epoch 192/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6733 - accuracy: 0.7714 - val_loss: 0.6120 - val_accuracy: 0.7952\n",
            "Epoch 193/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6520 - accuracy: 0.7781 - val_loss: 0.6045 - val_accuracy: 0.7938\n",
            "Epoch 194/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6622 - accuracy: 0.7739 - val_loss: 0.6323 - val_accuracy: 0.7888\n",
            "Epoch 195/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6600 - accuracy: 0.7739 - val_loss: 0.6073 - val_accuracy: 0.7954\n",
            "Epoch 196/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6614 - accuracy: 0.7752 - val_loss: 0.5968 - val_accuracy: 0.8034\n",
            "Epoch 197/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6695 - accuracy: 0.7728 - val_loss: 0.6152 - val_accuracy: 0.7912\n",
            "Epoch 198/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6631 - accuracy: 0.7743 - val_loss: 0.6204 - val_accuracy: 0.7894\n",
            "Epoch 199/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6661 - accuracy: 0.7722 - val_loss: 0.5962 - val_accuracy: 0.7982\n",
            "Epoch 200/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6589 - accuracy: 0.7773 - val_loss: 0.6147 - val_accuracy: 0.7958\n",
            "Epoch 201/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6590 - accuracy: 0.7771 - val_loss: 0.6076 - val_accuracy: 0.7938\n",
            "Epoch 202/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6609 - accuracy: 0.7743 - val_loss: 0.6175 - val_accuracy: 0.7912\n",
            "Epoch 203/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6641 - accuracy: 0.7742 - val_loss: 0.6170 - val_accuracy: 0.7916\n",
            "Epoch 204/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6617 - accuracy: 0.7735 - val_loss: 0.6017 - val_accuracy: 0.7982\n",
            "Epoch 205/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6611 - accuracy: 0.7731 - val_loss: 0.6125 - val_accuracy: 0.7922\n",
            "Epoch 206/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6562 - accuracy: 0.7771 - val_loss: 0.6118 - val_accuracy: 0.7944\n",
            "Epoch 207/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6526 - accuracy: 0.7785 - val_loss: 0.6087 - val_accuracy: 0.7958\n",
            "Epoch 208/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6588 - accuracy: 0.7762 - val_loss: 0.6053 - val_accuracy: 0.8026\n",
            "Epoch 209/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6520 - accuracy: 0.7789 - val_loss: 0.6057 - val_accuracy: 0.7928\n",
            "Epoch 210/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6536 - accuracy: 0.7769 - val_loss: 0.6083 - val_accuracy: 0.7954\n",
            "Epoch 211/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6476 - accuracy: 0.7774 - val_loss: 0.6395 - val_accuracy: 0.7848\n",
            "Epoch 212/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6507 - accuracy: 0.7792 - val_loss: 0.6200 - val_accuracy: 0.7862\n",
            "Epoch 213/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6480 - accuracy: 0.7800 - val_loss: 0.6127 - val_accuracy: 0.7844\n",
            "Epoch 214/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6640 - accuracy: 0.7761 - val_loss: 0.6189 - val_accuracy: 0.7898\n",
            "Epoch 215/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6558 - accuracy: 0.7764 - val_loss: 0.6050 - val_accuracy: 0.7972\n",
            "Epoch 216/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6455 - accuracy: 0.7788 - val_loss: 0.6060 - val_accuracy: 0.7976\n",
            "Epoch 217/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6480 - accuracy: 0.7786 - val_loss: 0.5952 - val_accuracy: 0.7994\n",
            "Epoch 218/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6451 - accuracy: 0.7800 - val_loss: 0.6082 - val_accuracy: 0.8010\n",
            "Epoch 219/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6457 - accuracy: 0.7793 - val_loss: 0.6063 - val_accuracy: 0.7920\n",
            "Epoch 220/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6498 - accuracy: 0.7797 - val_loss: 0.6001 - val_accuracy: 0.7970\n",
            "Epoch 221/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6485 - accuracy: 0.7786 - val_loss: 0.6026 - val_accuracy: 0.7962\n",
            "Epoch 222/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6532 - accuracy: 0.7786 - val_loss: 0.5990 - val_accuracy: 0.8004\n",
            "Epoch 223/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6511 - accuracy: 0.7793 - val_loss: 0.6026 - val_accuracy: 0.7964\n",
            "Epoch 224/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6447 - accuracy: 0.7792 - val_loss: 0.6032 - val_accuracy: 0.7964\n",
            "Epoch 225/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6445 - accuracy: 0.7805 - val_loss: 0.6050 - val_accuracy: 0.7950\n",
            "Epoch 226/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6463 - accuracy: 0.7804 - val_loss: 0.6049 - val_accuracy: 0.7948\n",
            "Epoch 227/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6414 - accuracy: 0.7813 - val_loss: 0.6107 - val_accuracy: 0.7940\n",
            "Epoch 228/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6495 - accuracy: 0.7787 - val_loss: 0.6027 - val_accuracy: 0.7950\n",
            "Epoch 229/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6389 - accuracy: 0.7841 - val_loss: 0.5942 - val_accuracy: 0.7980\n",
            "Epoch 230/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6457 - accuracy: 0.7795 - val_loss: 0.5869 - val_accuracy: 0.8034\n",
            "Epoch 231/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6437 - accuracy: 0.7776 - val_loss: 0.6105 - val_accuracy: 0.8020\n",
            "Epoch 232/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6446 - accuracy: 0.7822 - val_loss: 0.5947 - val_accuracy: 0.8000\n",
            "Epoch 233/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6394 - accuracy: 0.7842 - val_loss: 0.6000 - val_accuracy: 0.7966\n",
            "Epoch 234/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6330 - accuracy: 0.7846 - val_loss: 0.5932 - val_accuracy: 0.7974\n",
            "Epoch 235/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6440 - accuracy: 0.7806 - val_loss: 0.5932 - val_accuracy: 0.8050\n",
            "Epoch 236/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6361 - accuracy: 0.7829 - val_loss: 0.6048 - val_accuracy: 0.7952\n",
            "Epoch 237/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6433 - accuracy: 0.7804 - val_loss: 0.5903 - val_accuracy: 0.8046\n",
            "Epoch 238/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6331 - accuracy: 0.7824 - val_loss: 0.6042 - val_accuracy: 0.7950\n",
            "Epoch 239/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6391 - accuracy: 0.7839 - val_loss: 0.6057 - val_accuracy: 0.7978\n",
            "Epoch 240/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6384 - accuracy: 0.7803 - val_loss: 0.6012 - val_accuracy: 0.7938\n",
            "Epoch 241/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6365 - accuracy: 0.7829 - val_loss: 0.5977 - val_accuracy: 0.7948\n",
            "Epoch 242/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6278 - accuracy: 0.7847 - val_loss: 0.5868 - val_accuracy: 0.8036\n",
            "Epoch 243/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6316 - accuracy: 0.7843 - val_loss: 0.6083 - val_accuracy: 0.7948\n",
            "Epoch 244/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6294 - accuracy: 0.7850 - val_loss: 0.5795 - val_accuracy: 0.8038\n",
            "Epoch 245/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6363 - accuracy: 0.7816 - val_loss: 0.6046 - val_accuracy: 0.7976\n",
            "Epoch 246/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6255 - accuracy: 0.7865 - val_loss: 0.5947 - val_accuracy: 0.8022\n",
            "Epoch 247/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6334 - accuracy: 0.7847 - val_loss: 0.5980 - val_accuracy: 0.7976\n",
            "Epoch 248/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6296 - accuracy: 0.7883 - val_loss: 0.5869 - val_accuracy: 0.8030\n",
            "Epoch 249/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6212 - accuracy: 0.7889 - val_loss: 0.5825 - val_accuracy: 0.8054\n",
            "Epoch 250/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6311 - accuracy: 0.7859 - val_loss: 0.5875 - val_accuracy: 0.8030\n",
            "Epoch 251/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6297 - accuracy: 0.7874 - val_loss: 0.6022 - val_accuracy: 0.7982\n",
            "Epoch 252/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6331 - accuracy: 0.7864 - val_loss: 0.5832 - val_accuracy: 0.8052\n",
            "Epoch 253/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6273 - accuracy: 0.7857 - val_loss: 0.6045 - val_accuracy: 0.7946\n",
            "Epoch 254/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6295 - accuracy: 0.7864 - val_loss: 0.6044 - val_accuracy: 0.7906\n",
            "Epoch 255/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6305 - accuracy: 0.7891 - val_loss: 0.5942 - val_accuracy: 0.7960\n",
            "Epoch 256/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6338 - accuracy: 0.7844 - val_loss: 0.5922 - val_accuracy: 0.8008\n",
            "Epoch 257/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6333 - accuracy: 0.7862 - val_loss: 0.5860 - val_accuracy: 0.8016\n",
            "Epoch 258/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6231 - accuracy: 0.7877 - val_loss: 0.5813 - val_accuracy: 0.8026\n",
            "Epoch 259/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6305 - accuracy: 0.7836 - val_loss: 0.6059 - val_accuracy: 0.7906\n",
            "Epoch 260/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6251 - accuracy: 0.7870 - val_loss: 0.5943 - val_accuracy: 0.7972\n",
            "Epoch 261/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6262 - accuracy: 0.7864 - val_loss: 0.5953 - val_accuracy: 0.7980\n",
            "Epoch 262/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6307 - accuracy: 0.7841 - val_loss: 0.6014 - val_accuracy: 0.7990\n",
            "Epoch 263/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6215 - accuracy: 0.7885 - val_loss: 0.5912 - val_accuracy: 0.7978\n",
            "Epoch 264/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6181 - accuracy: 0.7899 - val_loss: 0.5801 - val_accuracy: 0.7994\n",
            "Epoch 265/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6288 - accuracy: 0.7854 - val_loss: 0.6003 - val_accuracy: 0.8012\n",
            "Epoch 266/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6230 - accuracy: 0.7874 - val_loss: 0.5839 - val_accuracy: 0.8038\n",
            "Epoch 267/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6341 - accuracy: 0.7837 - val_loss: 0.5997 - val_accuracy: 0.7984\n",
            "Epoch 268/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6242 - accuracy: 0.7877 - val_loss: 0.5980 - val_accuracy: 0.8006\n",
            "Epoch 269/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6248 - accuracy: 0.7868 - val_loss: 0.5975 - val_accuracy: 0.7974\n",
            "Epoch 270/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6317 - accuracy: 0.7866 - val_loss: 0.6050 - val_accuracy: 0.7956\n",
            "Epoch 271/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6230 - accuracy: 0.7891 - val_loss: 0.5931 - val_accuracy: 0.8014\n",
            "Epoch 272/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6191 - accuracy: 0.7888 - val_loss: 0.6060 - val_accuracy: 0.7944\n",
            "Epoch 273/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6127 - accuracy: 0.7891 - val_loss: 0.5952 - val_accuracy: 0.7972\n",
            "Epoch 274/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6202 - accuracy: 0.7894 - val_loss: 0.6107 - val_accuracy: 0.7956\n",
            "Epoch 275/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6175 - accuracy: 0.7924 - val_loss: 0.5941 - val_accuracy: 0.8016\n",
            "Epoch 276/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6260 - accuracy: 0.7883 - val_loss: 0.5897 - val_accuracy: 0.7978\n",
            "Epoch 277/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6092 - accuracy: 0.7920 - val_loss: 0.5872 - val_accuracy: 0.8024\n",
            "Epoch 278/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6239 - accuracy: 0.7867 - val_loss: 0.6103 - val_accuracy: 0.7958\n",
            "Epoch 279/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6215 - accuracy: 0.7896 - val_loss: 0.6023 - val_accuracy: 0.7982\n",
            "Epoch 280/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6236 - accuracy: 0.7865 - val_loss: 0.6052 - val_accuracy: 0.7978\n",
            "Epoch 281/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6156 - accuracy: 0.7923 - val_loss: 0.6068 - val_accuracy: 0.7932\n",
            "Epoch 282/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6163 - accuracy: 0.7891 - val_loss: 0.5914 - val_accuracy: 0.8024\n",
            "Epoch 283/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6249 - accuracy: 0.7893 - val_loss: 0.5986 - val_accuracy: 0.8000\n",
            "Epoch 284/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6250 - accuracy: 0.7849 - val_loss: 0.6012 - val_accuracy: 0.8024\n",
            "Epoch 285/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6128 - accuracy: 0.7896 - val_loss: 0.5798 - val_accuracy: 0.8072\n",
            "Epoch 286/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6176 - accuracy: 0.7915 - val_loss: 0.5896 - val_accuracy: 0.7992\n",
            "Epoch 287/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6199 - accuracy: 0.7902 - val_loss: 0.5819 - val_accuracy: 0.8056\n",
            "Epoch 288/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6088 - accuracy: 0.7927 - val_loss: 0.6001 - val_accuracy: 0.7948\n",
            "Epoch 289/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6089 - accuracy: 0.7922 - val_loss: 0.6089 - val_accuracy: 0.7930\n",
            "Epoch 290/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6192 - accuracy: 0.7906 - val_loss: 0.5879 - val_accuracy: 0.8028\n",
            "Epoch 291/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6105 - accuracy: 0.7939 - val_loss: 0.5883 - val_accuracy: 0.7990\n",
            "Epoch 292/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6103 - accuracy: 0.7924 - val_loss: 0.5842 - val_accuracy: 0.8030\n",
            "Epoch 293/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6074 - accuracy: 0.7914 - val_loss: 0.5998 - val_accuracy: 0.7968\n",
            "Epoch 294/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6232 - accuracy: 0.7884 - val_loss: 0.6036 - val_accuracy: 0.7948\n",
            "Epoch 295/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6118 - accuracy: 0.7911 - val_loss: 0.5992 - val_accuracy: 0.8020\n",
            "Epoch 296/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6137 - accuracy: 0.7900 - val_loss: 0.6058 - val_accuracy: 0.7954\n",
            "Epoch 297/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6061 - accuracy: 0.7923 - val_loss: 0.5943 - val_accuracy: 0.8036\n",
            "Epoch 298/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6182 - accuracy: 0.7906 - val_loss: 0.5822 - val_accuracy: 0.8074\n",
            "Epoch 299/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6077 - accuracy: 0.7940 - val_loss: 0.5840 - val_accuracy: 0.7960\n",
            "Epoch 300/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6131 - accuracy: 0.7922 - val_loss: 0.5933 - val_accuracy: 0.8020\n",
            "Epoch 301/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6069 - accuracy: 0.7935 - val_loss: 0.5932 - val_accuracy: 0.8020\n",
            "Epoch 302/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6164 - accuracy: 0.7911 - val_loss: 0.6074 - val_accuracy: 0.7920\n",
            "Epoch 303/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6120 - accuracy: 0.7908 - val_loss: 0.5917 - val_accuracy: 0.7990\n",
            "Epoch 304/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6153 - accuracy: 0.7912 - val_loss: 0.5895 - val_accuracy: 0.8012\n",
            "Epoch 305/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6022 - accuracy: 0.7952 - val_loss: 0.5917 - val_accuracy: 0.8038\n",
            "Epoch 306/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6137 - accuracy: 0.7927 - val_loss: 0.5844 - val_accuracy: 0.8076\n",
            "Epoch 307/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6050 - accuracy: 0.7949 - val_loss: 0.5806 - val_accuracy: 0.7988\n",
            "Epoch 308/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6108 - accuracy: 0.7911 - val_loss: 0.5943 - val_accuracy: 0.8060\n",
            "Epoch 309/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6051 - accuracy: 0.7952 - val_loss: 0.5901 - val_accuracy: 0.8024\n",
            "Epoch 310/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6143 - accuracy: 0.7935 - val_loss: 0.5870 - val_accuracy: 0.8020\n",
            "Epoch 311/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.6038 - accuracy: 0.7938 - val_loss: 0.5877 - val_accuracy: 0.8050\n",
            "Epoch 312/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6087 - accuracy: 0.7926 - val_loss: 0.5993 - val_accuracy: 0.7944\n",
            "Epoch 313/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6044 - accuracy: 0.7963 - val_loss: 0.5869 - val_accuracy: 0.8006\n",
            "Epoch 314/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6012 - accuracy: 0.7963 - val_loss: 0.5857 - val_accuracy: 0.8064\n",
            "Epoch 315/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6005 - accuracy: 0.7958 - val_loss: 0.6145 - val_accuracy: 0.7938\n",
            "Epoch 316/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5981 - accuracy: 0.7982 - val_loss: 0.5792 - val_accuracy: 0.8070\n",
            "Epoch 317/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5974 - accuracy: 0.7953 - val_loss: 0.5952 - val_accuracy: 0.7982\n",
            "Epoch 318/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6043 - accuracy: 0.7955 - val_loss: 0.5946 - val_accuracy: 0.7992\n",
            "Epoch 319/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6110 - accuracy: 0.7928 - val_loss: 0.5963 - val_accuracy: 0.8018\n",
            "Epoch 320/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5979 - accuracy: 0.7953 - val_loss: 0.5744 - val_accuracy: 0.8036\n",
            "Epoch 321/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6104 - accuracy: 0.7944 - val_loss: 0.6009 - val_accuracy: 0.7932\n",
            "Epoch 322/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6005 - accuracy: 0.7968 - val_loss: 0.5820 - val_accuracy: 0.8034\n",
            "Epoch 323/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6057 - accuracy: 0.7929 - val_loss: 0.6028 - val_accuracy: 0.7976\n",
            "Epoch 324/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6013 - accuracy: 0.7977 - val_loss: 0.5960 - val_accuracy: 0.7972\n",
            "Epoch 325/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6022 - accuracy: 0.7966 - val_loss: 0.5957 - val_accuracy: 0.8014\n",
            "Epoch 326/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5971 - accuracy: 0.7977 - val_loss: 0.5957 - val_accuracy: 0.8026\n",
            "Epoch 327/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6063 - accuracy: 0.7955 - val_loss: 0.5881 - val_accuracy: 0.8008\n",
            "Epoch 328/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6055 - accuracy: 0.7936 - val_loss: 0.5894 - val_accuracy: 0.7980\n",
            "Epoch 329/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6001 - accuracy: 0.7952 - val_loss: 0.5777 - val_accuracy: 0.8028\n",
            "Epoch 330/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5966 - accuracy: 0.7973 - val_loss: 0.5671 - val_accuracy: 0.8092\n",
            "Epoch 331/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5949 - accuracy: 0.7973 - val_loss: 0.5686 - val_accuracy: 0.8052\n",
            "Epoch 332/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6057 - accuracy: 0.7952 - val_loss: 0.5827 - val_accuracy: 0.8014\n",
            "Epoch 333/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6083 - accuracy: 0.7954 - val_loss: 0.5851 - val_accuracy: 0.8034\n",
            "Epoch 334/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5969 - accuracy: 0.7975 - val_loss: 0.5812 - val_accuracy: 0.8026\n",
            "Epoch 335/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6020 - accuracy: 0.7950 - val_loss: 0.5927 - val_accuracy: 0.7996\n",
            "Epoch 336/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6000 - accuracy: 0.7974 - val_loss: 0.5847 - val_accuracy: 0.7986\n",
            "Epoch 337/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5941 - accuracy: 0.7993 - val_loss: 0.6089 - val_accuracy: 0.8002\n",
            "Epoch 338/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5951 - accuracy: 0.7987 - val_loss: 0.6074 - val_accuracy: 0.7950\n",
            "Epoch 339/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5955 - accuracy: 0.7991 - val_loss: 0.5774 - val_accuracy: 0.8056\n",
            "Epoch 340/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5976 - accuracy: 0.7991 - val_loss: 0.5890 - val_accuracy: 0.8008\n",
            "Epoch 341/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5987 - accuracy: 0.7953 - val_loss: 0.5817 - val_accuracy: 0.8046\n",
            "Epoch 342/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6046 - accuracy: 0.7950 - val_loss: 0.6006 - val_accuracy: 0.7948\n",
            "Epoch 343/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5883 - accuracy: 0.8005 - val_loss: 0.5798 - val_accuracy: 0.8022\n",
            "Epoch 344/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5940 - accuracy: 0.7999 - val_loss: 0.5829 - val_accuracy: 0.8000\n",
            "Epoch 345/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5909 - accuracy: 0.7997 - val_loss: 0.5721 - val_accuracy: 0.8030\n",
            "Epoch 346/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5903 - accuracy: 0.7968 - val_loss: 0.5840 - val_accuracy: 0.8038\n",
            "Epoch 347/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5943 - accuracy: 0.7976 - val_loss: 0.5826 - val_accuracy: 0.7994\n",
            "Epoch 348/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5947 - accuracy: 0.7972 - val_loss: 0.5771 - val_accuracy: 0.7998\n",
            "Epoch 349/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5990 - accuracy: 0.7980 - val_loss: 0.5830 - val_accuracy: 0.7998\n",
            "Epoch 350/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5878 - accuracy: 0.8015 - val_loss: 0.5900 - val_accuracy: 0.7988\n",
            "Epoch 351/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6065 - accuracy: 0.7937 - val_loss: 0.5901 - val_accuracy: 0.7984\n",
            "Epoch 352/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6018 - accuracy: 0.7958 - val_loss: 0.5790 - val_accuracy: 0.7992\n",
            "Epoch 353/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5846 - accuracy: 0.8003 - val_loss: 0.5822 - val_accuracy: 0.8014\n",
            "Epoch 354/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5899 - accuracy: 0.7984 - val_loss: 0.5927 - val_accuracy: 0.8010\n",
            "Epoch 355/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5930 - accuracy: 0.7990 - val_loss: 0.5837 - val_accuracy: 0.8034\n",
            "Epoch 356/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5903 - accuracy: 0.8006 - val_loss: 0.5789 - val_accuracy: 0.8064\n",
            "Epoch 357/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.6035 - accuracy: 0.7946 - val_loss: 0.5782 - val_accuracy: 0.8044\n",
            "Epoch 358/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5842 - accuracy: 0.8035 - val_loss: 0.5849 - val_accuracy: 0.8004\n",
            "Epoch 359/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5921 - accuracy: 0.7993 - val_loss: 0.5909 - val_accuracy: 0.7984\n",
            "Epoch 360/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5898 - accuracy: 0.7987 - val_loss: 0.5666 - val_accuracy: 0.8042\n",
            "Epoch 361/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5975 - accuracy: 0.7983 - val_loss: 0.5766 - val_accuracy: 0.8026\n",
            "Epoch 362/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5908 - accuracy: 0.7981 - val_loss: 0.5850 - val_accuracy: 0.7998\n",
            "Epoch 363/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5939 - accuracy: 0.7985 - val_loss: 0.5742 - val_accuracy: 0.8060\n",
            "Epoch 364/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5814 - accuracy: 0.8036 - val_loss: 0.5824 - val_accuracy: 0.8032\n",
            "Epoch 365/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5902 - accuracy: 0.7989 - val_loss: 0.5808 - val_accuracy: 0.8036\n",
            "Epoch 366/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5904 - accuracy: 0.7985 - val_loss: 0.5687 - val_accuracy: 0.8114\n",
            "Epoch 367/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5886 - accuracy: 0.8011 - val_loss: 0.5863 - val_accuracy: 0.7960\n",
            "Epoch 368/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5887 - accuracy: 0.8005 - val_loss: 0.5726 - val_accuracy: 0.8066\n",
            "Epoch 369/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5952 - accuracy: 0.7995 - val_loss: 0.5683 - val_accuracy: 0.8058\n",
            "Epoch 370/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5778 - accuracy: 0.8061 - val_loss: 0.5935 - val_accuracy: 0.7976\n",
            "Epoch 371/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5761 - accuracy: 0.8047 - val_loss: 0.5961 - val_accuracy: 0.7986\n",
            "Epoch 372/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5979 - accuracy: 0.7990 - val_loss: 0.5771 - val_accuracy: 0.8006\n",
            "Epoch 373/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5889 - accuracy: 0.8025 - val_loss: 0.5931 - val_accuracy: 0.7978\n",
            "Epoch 374/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5765 - accuracy: 0.8057 - val_loss: 0.5866 - val_accuracy: 0.8012\n",
            "Epoch 375/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5821 - accuracy: 0.8038 - val_loss: 0.5772 - val_accuracy: 0.8078\n",
            "Epoch 376/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5846 - accuracy: 0.8040 - val_loss: 0.5689 - val_accuracy: 0.8090\n",
            "Epoch 377/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5881 - accuracy: 0.8011 - val_loss: 0.5747 - val_accuracy: 0.8034\n",
            "Epoch 378/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5855 - accuracy: 0.8013 - val_loss: 0.5852 - val_accuracy: 0.7972\n",
            "Epoch 379/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5884 - accuracy: 0.7994 - val_loss: 0.5862 - val_accuracy: 0.8040\n",
            "Epoch 380/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5795 - accuracy: 0.8043 - val_loss: 0.5768 - val_accuracy: 0.8046\n",
            "Epoch 381/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5873 - accuracy: 0.8002 - val_loss: 0.5771 - val_accuracy: 0.8018\n",
            "Epoch 382/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5941 - accuracy: 0.7985 - val_loss: 0.5752 - val_accuracy: 0.8026\n",
            "Epoch 383/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5791 - accuracy: 0.8028 - val_loss: 0.5883 - val_accuracy: 0.7970\n",
            "Epoch 384/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5822 - accuracy: 0.8015 - val_loss: 0.5799 - val_accuracy: 0.8026\n",
            "Epoch 385/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5826 - accuracy: 0.8041 - val_loss: 0.5773 - val_accuracy: 0.8060\n",
            "Epoch 386/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5825 - accuracy: 0.8029 - val_loss: 0.5766 - val_accuracy: 0.8042\n",
            "Epoch 387/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5824 - accuracy: 0.8036 - val_loss: 0.5675 - val_accuracy: 0.8070\n",
            "Epoch 388/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5773 - accuracy: 0.8033 - val_loss: 0.5729 - val_accuracy: 0.8022\n",
            "Epoch 389/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5808 - accuracy: 0.8049 - val_loss: 0.5855 - val_accuracy: 0.8026\n",
            "Epoch 390/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5843 - accuracy: 0.8005 - val_loss: 0.5915 - val_accuracy: 0.8028\n",
            "Epoch 391/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5816 - accuracy: 0.8036 - val_loss: 0.5740 - val_accuracy: 0.8090\n",
            "Epoch 392/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5798 - accuracy: 0.8049 - val_loss: 0.5692 - val_accuracy: 0.8048\n",
            "Epoch 393/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5800 - accuracy: 0.8025 - val_loss: 0.5790 - val_accuracy: 0.8024\n",
            "Epoch 394/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5810 - accuracy: 0.8025 - val_loss: 0.6130 - val_accuracy: 0.7922\n",
            "Epoch 395/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5906 - accuracy: 0.8016 - val_loss: 0.5725 - val_accuracy: 0.8050\n",
            "Epoch 396/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5805 - accuracy: 0.8048 - val_loss: 0.5730 - val_accuracy: 0.8040\n",
            "Epoch 397/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5819 - accuracy: 0.8036 - val_loss: 0.5775 - val_accuracy: 0.8024\n",
            "Epoch 398/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5789 - accuracy: 0.8045 - val_loss: 0.5803 - val_accuracy: 0.8040\n",
            "Epoch 399/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5775 - accuracy: 0.8038 - val_loss: 0.5766 - val_accuracy: 0.8060\n",
            "Epoch 400/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5854 - accuracy: 0.8018 - val_loss: 0.5761 - val_accuracy: 0.8084\n",
            "Epoch 401/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5775 - accuracy: 0.8039 - val_loss: 0.5742 - val_accuracy: 0.8048\n",
            "Epoch 402/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5772 - accuracy: 0.8036 - val_loss: 0.5733 - val_accuracy: 0.8038\n",
            "Epoch 403/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5809 - accuracy: 0.8064 - val_loss: 0.5785 - val_accuracy: 0.8040\n",
            "Epoch 404/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5770 - accuracy: 0.8048 - val_loss: 0.5723 - val_accuracy: 0.8044\n",
            "Epoch 405/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5714 - accuracy: 0.8072 - val_loss: 0.5699 - val_accuracy: 0.8106\n",
            "Epoch 406/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5774 - accuracy: 0.8050 - val_loss: 0.5748 - val_accuracy: 0.8020\n",
            "Epoch 407/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5739 - accuracy: 0.8071 - val_loss: 0.5789 - val_accuracy: 0.8034\n",
            "Epoch 408/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5753 - accuracy: 0.8025 - val_loss: 0.5737 - val_accuracy: 0.8056\n",
            "Epoch 409/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5844 - accuracy: 0.8022 - val_loss: 0.5704 - val_accuracy: 0.8054\n",
            "Epoch 410/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5790 - accuracy: 0.8051 - val_loss: 0.5761 - val_accuracy: 0.8016\n",
            "Epoch 411/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5778 - accuracy: 0.8067 - val_loss: 0.5915 - val_accuracy: 0.7972\n",
            "Epoch 412/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5715 - accuracy: 0.8067 - val_loss: 0.5825 - val_accuracy: 0.8032\n",
            "Epoch 413/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5756 - accuracy: 0.8051 - val_loss: 0.5704 - val_accuracy: 0.8046\n",
            "Epoch 414/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5732 - accuracy: 0.8072 - val_loss: 0.5724 - val_accuracy: 0.8046\n",
            "Epoch 415/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5794 - accuracy: 0.8048 - val_loss: 0.5738 - val_accuracy: 0.8036\n",
            "Epoch 416/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5737 - accuracy: 0.8090 - val_loss: 0.5759 - val_accuracy: 0.8068\n",
            "Epoch 417/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5841 - accuracy: 0.8046 - val_loss: 0.5763 - val_accuracy: 0.8042\n",
            "Epoch 418/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5757 - accuracy: 0.8056 - val_loss: 0.5721 - val_accuracy: 0.8014\n",
            "Epoch 419/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5721 - accuracy: 0.8081 - val_loss: 0.5806 - val_accuracy: 0.8018\n",
            "Epoch 420/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5769 - accuracy: 0.8077 - val_loss: 0.5910 - val_accuracy: 0.7992\n",
            "Epoch 421/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5690 - accuracy: 0.8058 - val_loss: 0.5717 - val_accuracy: 0.8072\n",
            "Epoch 422/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5657 - accuracy: 0.8079 - val_loss: 0.5751 - val_accuracy: 0.8038\n",
            "Epoch 423/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5776 - accuracy: 0.8042 - val_loss: 0.5745 - val_accuracy: 0.7988\n",
            "Epoch 424/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5721 - accuracy: 0.8067 - val_loss: 0.5651 - val_accuracy: 0.8086\n",
            "Epoch 425/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5769 - accuracy: 0.8053 - val_loss: 0.5760 - val_accuracy: 0.7998\n",
            "Epoch 426/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5833 - accuracy: 0.8026 - val_loss: 0.5782 - val_accuracy: 0.7968\n",
            "Epoch 427/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5812 - accuracy: 0.8040 - val_loss: 0.5759 - val_accuracy: 0.8066\n",
            "Epoch 428/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5727 - accuracy: 0.8056 - val_loss: 0.6141 - val_accuracy: 0.7944\n",
            "Epoch 429/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5774 - accuracy: 0.8043 - val_loss: 0.6144 - val_accuracy: 0.7890\n",
            "Epoch 430/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5651 - accuracy: 0.8103 - val_loss: 0.5708 - val_accuracy: 0.8044\n",
            "Epoch 431/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5719 - accuracy: 0.8077 - val_loss: 0.5956 - val_accuracy: 0.7964\n",
            "Epoch 432/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5641 - accuracy: 0.8077 - val_loss: 0.5744 - val_accuracy: 0.8038\n",
            "Epoch 433/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5640 - accuracy: 0.8105 - val_loss: 0.5718 - val_accuracy: 0.8058\n",
            "Epoch 434/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5746 - accuracy: 0.8050 - val_loss: 0.5781 - val_accuracy: 0.8032\n",
            "Epoch 435/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5718 - accuracy: 0.8072 - val_loss: 0.5661 - val_accuracy: 0.8070\n",
            "Epoch 436/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5711 - accuracy: 0.8085 - val_loss: 0.5720 - val_accuracy: 0.8036\n",
            "Epoch 437/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5716 - accuracy: 0.8062 - val_loss: 0.5919 - val_accuracy: 0.7958\n",
            "Epoch 438/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5737 - accuracy: 0.8044 - val_loss: 0.5772 - val_accuracy: 0.8036\n",
            "Epoch 439/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5682 - accuracy: 0.8077 - val_loss: 0.5827 - val_accuracy: 0.8004\n",
            "Epoch 440/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5711 - accuracy: 0.8085 - val_loss: 0.5788 - val_accuracy: 0.8024\n",
            "Epoch 441/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5746 - accuracy: 0.8062 - val_loss: 0.5682 - val_accuracy: 0.8064\n",
            "Epoch 442/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5708 - accuracy: 0.8088 - val_loss: 0.5657 - val_accuracy: 0.8056\n",
            "Epoch 443/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5668 - accuracy: 0.8097 - val_loss: 0.5773 - val_accuracy: 0.8058\n",
            "Epoch 444/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5731 - accuracy: 0.8066 - val_loss: 0.5674 - val_accuracy: 0.8084\n",
            "Epoch 445/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5690 - accuracy: 0.8082 - val_loss: 0.5718 - val_accuracy: 0.8048\n",
            "Epoch 446/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5634 - accuracy: 0.8101 - val_loss: 0.5920 - val_accuracy: 0.7950\n",
            "Epoch 447/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5654 - accuracy: 0.8083 - val_loss: 0.5622 - val_accuracy: 0.8132\n",
            "Epoch 448/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5681 - accuracy: 0.8101 - val_loss: 0.5840 - val_accuracy: 0.8038\n",
            "Epoch 449/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5651 - accuracy: 0.8103 - val_loss: 0.5878 - val_accuracy: 0.7972\n",
            "Epoch 450/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5726 - accuracy: 0.8083 - val_loss: 0.5754 - val_accuracy: 0.8034\n",
            "Epoch 451/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5662 - accuracy: 0.8084 - val_loss: 0.5738 - val_accuracy: 0.8056\n",
            "Epoch 452/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5690 - accuracy: 0.8086 - val_loss: 0.5749 - val_accuracy: 0.8116\n",
            "Epoch 453/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5669 - accuracy: 0.8094 - val_loss: 0.5830 - val_accuracy: 0.8036\n",
            "Epoch 454/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5694 - accuracy: 0.8100 - val_loss: 0.5737 - val_accuracy: 0.8010\n",
            "Epoch 455/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5680 - accuracy: 0.8104 - val_loss: 0.5758 - val_accuracy: 0.8020\n",
            "Epoch 456/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5613 - accuracy: 0.8125 - val_loss: 0.5842 - val_accuracy: 0.8020\n",
            "Epoch 457/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5686 - accuracy: 0.8085 - val_loss: 0.5800 - val_accuracy: 0.8040\n",
            "Epoch 458/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5682 - accuracy: 0.8084 - val_loss: 0.5848 - val_accuracy: 0.8008\n",
            "Epoch 459/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5648 - accuracy: 0.8094 - val_loss: 0.5708 - val_accuracy: 0.8108\n",
            "Epoch 460/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5641 - accuracy: 0.8105 - val_loss: 0.5785 - val_accuracy: 0.8052\n",
            "Epoch 461/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5708 - accuracy: 0.8070 - val_loss: 0.5690 - val_accuracy: 0.8074\n",
            "Epoch 462/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5716 - accuracy: 0.8095 - val_loss: 0.5651 - val_accuracy: 0.8048\n",
            "Epoch 463/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5664 - accuracy: 0.8093 - val_loss: 0.5735 - val_accuracy: 0.8036\n",
            "Epoch 464/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5664 - accuracy: 0.8087 - val_loss: 0.5724 - val_accuracy: 0.8010\n",
            "Epoch 465/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5710 - accuracy: 0.8078 - val_loss: 0.5823 - val_accuracy: 0.8008\n",
            "Epoch 466/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5735 - accuracy: 0.8049 - val_loss: 0.6157 - val_accuracy: 0.7938\n",
            "Epoch 467/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5694 - accuracy: 0.8084 - val_loss: 0.5864 - val_accuracy: 0.8104\n",
            "Epoch 468/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5635 - accuracy: 0.8116 - val_loss: 0.5637 - val_accuracy: 0.8114\n",
            "Epoch 469/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5634 - accuracy: 0.8101 - val_loss: 0.5673 - val_accuracy: 0.8062\n",
            "Epoch 470/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5586 - accuracy: 0.8118 - val_loss: 0.5716 - val_accuracy: 0.8040\n",
            "Epoch 471/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5618 - accuracy: 0.8107 - val_loss: 0.5804 - val_accuracy: 0.8034\n",
            "Epoch 472/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5651 - accuracy: 0.8092 - val_loss: 0.5780 - val_accuracy: 0.8006\n",
            "Epoch 473/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5711 - accuracy: 0.8075 - val_loss: 0.5604 - val_accuracy: 0.8106\n",
            "Epoch 474/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5547 - accuracy: 0.8127 - val_loss: 0.5738 - val_accuracy: 0.8102\n",
            "Epoch 475/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5612 - accuracy: 0.8114 - val_loss: 0.5668 - val_accuracy: 0.8090\n",
            "Epoch 476/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5646 - accuracy: 0.8085 - val_loss: 0.5722 - val_accuracy: 0.8026\n",
            "Epoch 477/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5568 - accuracy: 0.8121 - val_loss: 0.5582 - val_accuracy: 0.8088\n",
            "Epoch 478/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5538 - accuracy: 0.8131 - val_loss: 0.5749 - val_accuracy: 0.8036\n",
            "Epoch 479/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5661 - accuracy: 0.8094 - val_loss: 0.5755 - val_accuracy: 0.8056\n",
            "Epoch 480/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5646 - accuracy: 0.8103 - val_loss: 0.5837 - val_accuracy: 0.7984\n",
            "Epoch 481/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5594 - accuracy: 0.8112 - val_loss: 0.5580 - val_accuracy: 0.8132\n",
            "Epoch 482/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5652 - accuracy: 0.8096 - val_loss: 0.5888 - val_accuracy: 0.7986\n",
            "Epoch 483/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5582 - accuracy: 0.8111 - val_loss: 0.5828 - val_accuracy: 0.8050\n",
            "Epoch 484/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5593 - accuracy: 0.8126 - val_loss: 0.5661 - val_accuracy: 0.8100\n",
            "Epoch 485/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5686 - accuracy: 0.8086 - val_loss: 0.5793 - val_accuracy: 0.8048\n",
            "Epoch 486/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5604 - accuracy: 0.8111 - val_loss: 0.6022 - val_accuracy: 0.7980\n",
            "Epoch 487/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5536 - accuracy: 0.8138 - val_loss: 0.5762 - val_accuracy: 0.8048\n",
            "Epoch 488/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5621 - accuracy: 0.8094 - val_loss: 0.5786 - val_accuracy: 0.7998\n",
            "Epoch 489/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5624 - accuracy: 0.8103 - val_loss: 0.5846 - val_accuracy: 0.7980\n",
            "Epoch 490/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5583 - accuracy: 0.8116 - val_loss: 0.5791 - val_accuracy: 0.8022\n",
            "Epoch 491/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5642 - accuracy: 0.8092 - val_loss: 0.5686 - val_accuracy: 0.8054\n",
            "Epoch 492/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5582 - accuracy: 0.8095 - val_loss: 0.5666 - val_accuracy: 0.8046\n",
            "Epoch 493/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5547 - accuracy: 0.8127 - val_loss: 0.5764 - val_accuracy: 0.8022\n",
            "Epoch 494/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5622 - accuracy: 0.8106 - val_loss: 0.5741 - val_accuracy: 0.8026\n",
            "Epoch 495/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5597 - accuracy: 0.8105 - val_loss: 0.5691 - val_accuracy: 0.8062\n",
            "Epoch 496/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5618 - accuracy: 0.8125 - val_loss: 0.5738 - val_accuracy: 0.8032\n",
            "Epoch 497/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5475 - accuracy: 0.8132 - val_loss: 0.5667 - val_accuracy: 0.8080\n",
            "Epoch 498/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5593 - accuracy: 0.8107 - val_loss: 0.5654 - val_accuracy: 0.8070\n",
            "Epoch 499/500\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.5550 - accuracy: 0.8121 - val_loss: 0.5704 - val_accuracy: 0.8056\n",
            "Epoch 500/500\n",
            "98/98 [==============================] - 3s 32ms/step - loss: 0.5544 - accuracy: 0.8134 - val_loss: 0.5713 - val_accuracy: 0.8066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:56:28.958057Z",
          "start_time": "2019-05-01T08:56:28.713031Z"
        },
        "id": "6GTGeChttNoi",
        "outputId": "2369a9e9-c916-4820-edaf-201b75c617ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def display_history(history):\n",
        "    \"\"\"Summarize history for accuracy and loss.\n",
        "    \"\"\"\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.show()\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "display_history(history);"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:52:07.228831Z",
          "start_time": "2019-05-01T08:52:07.222761Z"
        },
        "id": "O5UxwXY-KdFh"
      },
      "source": [
        "assert max(history.history['val_accuracy']) > 0.75"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvdz8y0LYVte",
        "outputId": "148b0489-d9e3-4865-c13e-fa69bf519ec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(test_images, test_labels, batch_size=1000)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate on test data\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5476 - accuracy: 0.8118\n",
            "test loss, test acc: [0.5476459860801697, 0.8118000030517578]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7nD81aQtNo3"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T08:52:10.803767Z",
          "start_time": "2019-05-01T08:52:10.663477Z"
        },
        "id": "XcFR3MkStNo5"
      },
      "source": [
        "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'"
      ],
      "execution_count": 63,
      "outputs": []
    }
  ]
}